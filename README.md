# Texas Hold'em Â· Q-Learning Lab

> A high-fidelity Python simulation of a **simplified heads-up Texas Hold'em MDP** (post-flop) solved with **tabular Q-Learning**.

![Python 3.9+](https://img.shields.io/badge/python-3.9%2B-blue)
![License MIT](https://img.shields.io/badge/license-MIT-green)

---

## Table of Contents

1. [Overview](#overview)
2. [Architecture (MVC)](#architecture-mvc)
3. [MDP Formulation](#mdp-formulation)
4. [Hand Ranking Engine](#hand-ranking-engine)
5. [Q-Learning Agent](#q-learning-agent)
6. [GUI Features](#gui-features)
7. [Installation](#installation)
8. [Usage](#usage)
9. [Convergence to Nash Equilibrium](#convergence-to-nash-equilibrium)
10. [File Descriptions](#file-descriptions)
11. [Customisation](#customisation)
12. [Troubleshooting](#troubleshooting)

---

## Overview

This project implements a simplified **heads-up** (1 v 1) Texas Hold'em starting from the **flop** with a fixed hero hand:

| Component | Value |
|-----------|-------|
| **Hero** | 8â™¥ 9â™¥ |
| **Flop** | Jâ™¥ Qâ™¥ 2â™£ |
| **Stacks** | $150 each |
| **Pot** | $100 (pre-flop action already concluded) |

Hero holds a **double-gutter straight draw** (needs T or K) **and** a **flush draw** (9 heart outs).  Combined unique outs â‰ˆ 15 cards, giving roughly **54 % equity** to improve by the river.

The opponent is modelled as a **fixed stochastic policy** (analogous to the dealer in Blackjack).  A **Q-Learning** agent learns, through thousands of episodes, to exploit this sub-game.

---

## Architecture (MVC)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          main.py           â”‚  â† Entry point
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚ imports
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           ui.py            â”‚  â† View + Controller (CustomTkinter)
â”‚  â€¢ Card animations         â”‚
â”‚  â€¢ Q-Table heatmap         â”‚
â”‚  â€¢ Win-rate live graph     â”‚
â”‚  â€¢ Human-vs-AI mode        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚ imports
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          agent.py          â”‚  â† Controller (Îµ-greedy Q-Learning)
â”‚  â€¢ Q-table management      â”‚
â”‚  â€¢ Îµ-greedy action select  â”‚
â”‚  â€¢ TD update rule          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚ imports
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         engine.py          â”‚  â† Model (MDP environment)
â”‚  â€¢ Card, HandEvaluator     â”‚
â”‚  â€¢ GameState (NamedTuple)  â”‚
â”‚  â€¢ PokerEnv (transitions)  â”‚
â”‚  â€¢ OpponentPolicy          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚ imports
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         config.py          â”‚  â† Constants & theme palette
â”‚  â€¢ Poker rules & defaults  â”‚
â”‚  â€¢ Q-Learning defaults     â”‚
â”‚  â€¢ GUI theme colours       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

Strict **separation of concerns** â€” the environment knows nothing about the GUI, the agent knows nothing about rendering, and the GUI orchestrates both.

---

## MDP Formulation

### State Space

```
S = (street, hero_stack, pot)
```

Where `street âˆˆ {flop, turn, river, showdown}`.  Community cards are implicitly encoded because the hero hand and flop are fixed (only the turn and river are stochastic).

### Action Space

```
A = {fold, call, raise_100, raise_150}
```

`raise_150` is an all-in.

### Transition Dynamics

After both players act on a street, the next community card is dealt **uniformly at random** from the 47 remaining unknown cards (52 âˆ’ 2 hero âˆ’ 3 flop).

```
P(s' | s, a)  âˆ  Uniform over remaining deck
```

### Reward Function

```
R = Î”Stack_hero = stack_final âˆ’ stack_initial
```

No intermediate shaping â€” the agent receives reward **only** when the hand concludes (fold, opponent fold, or showdown).

### Pot Splitting

On a tied showdown, the pot is split evenly.  **Odd chips go to the hero** (player left of the dealer), per BGC rules:

```python
hero_share = (pot + 1) // 2
```

---

## Hand Ranking Engine

Strictly follows the BGC Texas Hold'em hierarchy:

| Rank | Hand | Numeric |
|------|------|---------|
| 10 | Royal Flush | Aâ™ Kâ™ Qâ™ Jâ™ Tâ™  |
| 9 | Straight Flush | 9â™¥8â™¥7â™¥6â™¥5â™¥ |
| 8 | Four of a Kind | Kâ™ Kâ™¥Kâ™¦Kâ™£5â™  |
| 7 | Full House | Kâ™ Kâ™¥Kâ™¦5â™ 5â™¥ |
| 6 | Flush | Kâ™¥9â™¥7â™¥5â™¥2â™¥ |
| 5 | Straight | Kâ™ Qâ™¥Jâ™¦Tâ™£9â™  |
| 4 | Three of a Kind | 7â™ 7â™¥7â™¦Kâ™£2â™  |
| 3 | Two Pair | Kâ™ Kâ™¥7â™¦7â™£2â™  |
| 2 | One Pair | Aâ™ Aâ™¥7â™¦5â™£2â™  |
| 1 | High Card | Aâ™ Kâ™¥9â™¦5â™£2â™  |

### Ace Dynamics

The Ace plays **both** roles:

- **High**: A-K-Q-J-T straight (the Broadway)
- **Low**: 5-4-3-2-A straight (the Wheel)

The evaluator considers all C(7,5) = 21 five-card combinations from the 7-card pool and returns the best.

---

## Q-Learning Agent

### Update Rule

$$Q(s, a) \leftarrow Q(s, a) + \alpha \bigl[ r + \gamma \max_{a'} Q(s', a') - Q(s, a) \bigr]$$

### Exploration Strategy

**Îµ-greedy**: with probability Îµ choose a random valid action; otherwise choose the action with the highest Q-value (ties broken randomly).

### Hyperparameters

| Symbol | Parameter | Default | Range |
|--------|-----------|---------|-------|
| Î± | `learning_rate` | 0.10 | 0.01 â€“ 0.5 |
| Î³ | `discount_factor` | 0.95 | 0.90 â€“ 0.99 |
| Îµ | `epsilon` | 0.20 | 0.05 â€“ 0.40 |

All three are **exposed** in the GUI and in the constructor for programmatic tuning.

---

## GUI Features

Built with **CustomTkinter** for a modern dark-mode aesthetic.

| Feature | Description |
|---------|-------------|
| ðŸƒ **Card widgets** | Visual card rendering with suit colours & smooth deal animation |
| ðŸ“Š **Q-Table Heatmap** | Real-time colour-coded matrix (states Ã— actions) using RdYlGn colourmap |
| ðŸ“ˆ **Win-Rate Graph** | 50-episode rolling average updated after training |
| ðŸ“‰ **Reward Graph** | Raw + moving-average reward curve |
| ðŸ§  **AI Thought Process** | Live Q-value display for the current game state |
| ðŸŽ® **Human vs AI** | Play manually while seeing what the AI *would* choose |
| ðŸ¤– **Watch AI** | Step-by-step AI play with 1.2 s delays |
| âš™ï¸ **Hyperparameter Tuning** | Adjust Î±, Î³, Îµ from the GUI before training |
| ðŸ“Š **Progress Bar** | Real-time training progress indicator |

---

## Installation

### Prerequisites

- **Python 3.9+**
- **tkinter** (usually bundled with Python)

### Steps

```bash
# 1. Clone / navigate to the project
cd TexasHold\'em

# 2. (Optional) create a virtual environment
python -m venv .venv && source .venv/bin/activate

# 3. Install dependencies
pip install -r requirements.txt
```

If `tkinter` is missing on Linux:

```bash
sudo apt-get install python3-tk
```

---

## Usage

### GUI (recommended)

```bash
python main.py
```

1. **Train** â€” enter episode count & hyperparameters â†’ click *Start Training*.
2. **Inspect** â€” switch between the *Win Rate*, *Reward*, *Q-Table Heatmap*, and *Q-Values* tabs.
3. **Play** â€” click *New Game (Manual)* and use the action buttons.
4. **Watch** â€” click *Watch AI Play* to see the trained agent in action.

### Command-line demo

```bash
python demo.py
```

Runs the full validation suite: hand evaluator, pot splitting, outs analysis, training convergence, and demonstration games.

### Programmatic

```python
from engine import PokerEnv
from agent import QLearningAgent

env = PokerEnv()
agent = QLearningAgent(actions=env.actions, learning_rate=0.1,
                       discount_factor=0.95, epsilon=0.2)
agent.train(env, num_episodes=5000, verbose_every=1000)
agent.save("my_agent.pkl")

# Play one hand
state = env.reset()
done = False
while not done:
    action = agent.get_action(state, env.get_valid_actions(), training=False)
    result = env.step(action)
    state, reward, done, info = result

print(f"Winner: {info['winner']}  |  Reward: ${reward:+.0f}")
```

---

## Convergence to Nash Equilibrium

### Why This Sub-game Has a Clear Optimal Strategy

In this **fixed sub-game**, hero always starts with the same hand (8â™¥9â™¥) against a **stationary stochastic opponent**.  Because:

1. The opponent's policy is **fixed** (not adapting).
2. The state space is **finite** and **fully observable** to the agent.
3. Transitions are **Markovian** (next state depends only on current state + action + random card).

â€¦the Q-Learning algorithm is **guaranteed to converge** to the optimal Q-function $Q^*(s, a)$ as $t \to \infty$, provided:

- Every (state, action) pair is visited infinitely often (ensured by Îµ-greedy).
- The learning rate satisfies the Robbins-Monro conditions (constant Î± works in practice for finite MDPs).

### What the Agent Learns

| Street | Optimal Action | Reasoning |
|--------|---------------|-----------|
| **Flop** | Call / Raise | 15 outs â‰ˆ 54% equity.  Folding leaves money on the table. |
| **Turn** (hit) | Raise | Made hand (flush or straight).  Extract value. |
| **Turn** (miss) | Call | Still 15 outs with 1 card to come â‰ˆ 30% equity.  Pot odds justify calling. |
| **River** (hit) | Raise | Value bet the made hand. |
| **River** (miss) | Fold | No equity remaining.  Minimise losses. |

After 5 000+ episodes, the Q-values clearly reflect this pattern â€” `Q(flop, call) >> Q(flop, fold)` and `Q(river_miss, fold) > Q(river_miss, call)`.

### Approximation of Nash Equilibrium

Against a fixed opponent, the converged Q-policy is the **best response** to that opponent's strategy.  In two-player zero-sum games, a pair of best responses constitutes a **Nash Equilibrium**.  Since the opponent is fixed, the agent's converged policy is the NE *for this specific sub-game*.

For a truly adaptive opponent, one would need **fictitious play**, **CFR (Counterfactual Regret Minimisation)**, or **Nash-Q** â€” extensions left as future work.

---

## File Descriptions

| File | Role | Key Classes |
|------|------|-------------|
| [config.py](config.py) | **Config** â€” Constants & palette | All poker, RL, and theme constants |
| [engine.py](engine.py) | **Model** â€” MDP environment | `Card`, `HandEvaluator`, `HandRank`, `GameState`, `OpponentPolicy`, `PokerEnv` |
| [agent.py](agent.py) | **Controller** â€” RL agent | `QLearningAgent` |
| [ui.py](ui.py) | **View** â€” CustomTkinter GUI | `CardWidget`, `PokerGUI` |
| [main.py](main.py) | **Entry point** | â€” |
| [demo.py](demo.py) | CLI validation suite | `test_hand_evaluator`, `test_training`, â€¦ |
| [QUICKSTART.py](QUICKSTART.py) | Quick-start guide | â€” |
| [requirements.txt](requirements.txt) | Dependencies | â€” |

---

## Customisation

### Change starting hand

In `engine.py`:

```python
self.hero_cards = [Card("A", "s"), Card("K", "s")]
self.flop = [Card("A", "h"), Card("K", "h"), Card("Q", "h")]
```

### Adjust opponent aggression

```python
self.opponent_policy = OpponentPolicy(aggression=0.5, fold_prob=0.15)
```

### Tune hyperparameters

```python
agent = QLearningAgent(
    actions=env.actions,
    learning_rate=0.05,    # slower, more stable
    discount_factor=0.99,  # values future rewards more
    epsilon=0.10,          # less exploration
)
```

Or adjust them directly in the GUI before clicking *Start Training*.

---

## Troubleshooting

| Issue | Solution |
|-------|----------|
| `ModuleNotFoundError: customtkinter` | `pip install customtkinter` |
| `ModuleNotFoundError: tkinter` | `sudo apt-get install python3-tk` |
| GUI blank / crash | Ensure `matplotlib` backend is `TkAgg` (set automatically) |
| Agent not learning | Train â‰¥ 5 000 episodes; try Î±=0.05, Îµ=0.15 |
| Slow training | 5 000 episodes â‰ˆ 3 s; 50 000 â‰ˆ 30 s |

---

## Expected Results

| Episodes | Win Rate | Avg Reward | Q-Table States |
|----------|----------|------------|----------------|
| 1 000 | 40â€“50 % | $0â€“15 | 5â€“10 |
| 5 000 | 45â€“55 % | $10â€“25 | 10â€“20 |
| 20 000 | 50â€“60 % | $15â€“30 | 15â€“25 |

---

## License

MIT â€” see individual file headers.

---

*Built as a reinforcement-learning research prototype.  Not intended for real-money gambling.*
